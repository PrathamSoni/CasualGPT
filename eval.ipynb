{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import os\n",
    "from src.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_datasets()\n",
    "model = \"gpt-3.5-turbo\"\n",
    "cot_bool = False\n",
    "authority = \"\"\n",
    "verb = \"causes\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"\"\"You are an AI assistant for helping humans build causal diagrams. Assess the validity of the given causal relationship between two variables.\"\"\"\n",
    "\n",
    "human_template = '''The following statement expresses a causal relationship between two variables.{ans_format}\n",
    "----------------\n",
    "STATEMENT: {authority}{var1} {verb} {var2}.\n",
    "ANSWER:{cot}'''\n",
    "\n",
    "messages = [SystemMessagePromptTemplate.from_template(system_template), HumanMessagePromptTemplate.from_template(human_template)]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chat = LLMChain(llm=ChatOpenAI(model_name=model, temperature=0), prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant for helping humans build causal diagrams. Assess the validity of the given causal relationship between two variables.\n",
      "Human: The following statement expresses a causal relationship between two variables.\n",
      "Your answer should just be 'TRUE' if the causal relationship is true or 'FALSE' if the causal relationship is false.\n",
      "----------------\n",
      "STATEMENT: EIF3A causes Lung Neoplasms.\n",
      "ANSWER:\n",
      "System: You are an AI assistant for helping humans build causal diagrams. Assess the validity of the given causal relationship between two variables.\n",
      "Human: The following statement expresses a causal relationship between two variables.\n",
      "Your answer should just be 'TRUE' if the causal relationship is true or 'FALSE' if the causal relationship is false.\n",
      "----------------\n",
      "STATEMENT: acetaminophen glucuronide causes rs3130690.\n",
      "ANSWER:\n",
      "System: You are an AI assistant for helping humans build causal diagrams. Assess the validity of the given causal relationship between two variables.\n",
      "Human: The following statement expresses a causal relationship between two variables.\n",
      "Your answer should just be 'TRUE' if the causal relationship is true or 'FALSE' if the causal relationship is false.\n",
      "----------------\n",
      "STATEMENT: ARAP1 causes Hypertension.\n",
      "ANSWER:\n",
      "System: You are an AI assistant for helping humans build causal diagrams. Assess the validity of the given causal relationship between two variables.\n",
      "Human: The following statement expresses a causal relationship between two variables.\n",
      "Your answer should just be 'TRUE' if the causal relationship is true or 'FALSE' if the causal relationship is false.\n",
      "----------------\n",
      "STATEMENT: rs140471703 causes rs3957356.\n",
      "ANSWER:\n"
     ]
    }
   ],
   "source": [
    "#Generate answers to prompts\n",
    "examples = []\n",
    "predictions = examples[:]\n",
    "\n",
    "if cot_bool:\n",
    "    cot = '''\n",
    "Let's think step by step.'''\n",
    "    ans_format = '''\n",
    "Your answer should determine whether the causal relationship is true or false.'''\n",
    "else:\n",
    "    cot = ''\n",
    "    ans_format = '''\n",
    "Your answer should just be 'TRUE' if the causal relationship is true or 'FALSE' if the causal relationship is false.'''\n",
    "\n",
    "for i in range(2):#len(test[0])):\n",
    "    #TRUE\n",
    "    result = chat.run(cot=cot, ans_format=ans_format, authority=authority, var1=test[0][i][0], verb=verb, var2=test[0][i][1])\n",
    "    dict = {\n",
    "        \"query\" : \"{}{} {} {}\".format(authority, test[0][i][0], verb, test[0][i][1]),\n",
    "        \"answer\" : \"TRUE\",\n",
    "        \"result\" : result, \n",
    "        }\n",
    "    examples.append(dict)\n",
    "    #print(prompt.format(cot=cot, ans_format=ans_format, authority=authority, var1=test[0][i][0], verb=verb, var2=test[0][i][1]))\n",
    "    #FALSE\n",
    "    result = chat.run(cot=cot, ans_format=ans_format, authority=authority, var1=test[1][i][0], verb=verb, var2=test[1][i][1])\n",
    "    dict = {\n",
    "        \"query\" : \"{}{} {} {}\".format(authority, test[1][i][0], verb, test[1][i][1]),\n",
    "        \"answer\" : \"FALSE\",\n",
    "        \"result\" : result, \n",
    "        }\n",
    "    examples.append(dict)\n",
    "    #print(prompt.format(cot=cot, ans_format=ans_format, authority=authority, var1=test[1][i][0], verb=verb, var2=test[1][i][1]))\n",
    "\n",
    "predictions = examples[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up evaluation framework and evaluate results\n",
    "llm = ChatOpenAI(model_name=model, temperature=0)\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score it as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Please remember to grade them based on being factually accurate. If the STUDENT ANSWER has any conflicting answers, then the GRADE should be INCORRECT. If the STUDENT ANSWER has more than one answer, then the GRADE should be INCORRECT. If the STUDENT ANSWER does not clearly have just one correct answer, then the GRADE should be INCORRECT. Begin!\n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\", \"answer\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm, prompt=prompt)\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gpt-3.5-turbo\n",
      "Number of Examples : 4\n",
      "Verb : causes\n",
      "Authority : \n",
      "Uses CoT : False\n",
      "Overall Answer Accuracy : 0.5\n",
      "Recall : 0.0\n",
      "Specificity : 1.0\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(graded_outputs)):\n",
    "    if examples[i]['answer'] == 'TRUE':\n",
    "        if graded_outputs[i]['text'] == 'CORRECT':\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    else:\n",
    "        if graded_outputs[i]['text'] == 'CORRECT':\n",
    "            tn += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "results = [\n",
    "        {\"Model\": model, \n",
    "        \"Number of Examples\": len(examples),\n",
    "        \"Verb\": verb,\n",
    "        \"Authority\": authority, \n",
    "        \"Uses CoT\": cot_bool, \n",
    "        \"Overall Answer Accuracy\": (tp+tn)/(tp+tn+fp+fn), \n",
    "        \"Precision\": tp/(tp+fp),\n",
    "        \"Recall\": tp/(tp+fn),\n",
    "        \"Specificity\": tn/(tn+fp)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def export_to_csv(data, file_name):\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, \"a\", newline=\"\") as csvfile:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "export_to_csv(results, \"experiment_results.csv\")\n",
    "\n",
    "for i in range(len(results[0].keys())):\n",
    "    print(list(results[0].keys())[i], ':', list(results[0].values())[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
